# radisson - Example Configuration
#
# Copy this file to config.yaml and customize for your environment

server:
  host: "0.0.0.0"
  port: 8080
  request_timeout: 300
  # request_tracing: true  # Enable to track recent requests at /admin/requests

backends:
  # Remote OpenAI provider - gpt-3.5-turbo
  - id: "openai-gpt35"
    type: "remote"
    endpoint: "https://api.openai.com/v1"
    api_key: "<OPENAI_API_KEY>"  # Replace with api key
    model: "gpt-3.5-turbo"
    upstream_timeout: 60 # Optional, override default

  # Local backend - automatically loaded/unloaded according to available resources
  - id: "llama-3.1-1B"
    type: "local"
    command: "llama-server -m ./llama-3.1-1B.Q2_K.gguf --port {port} -c 4096 --host 127.0.0.1"
    resources:
      memory: "100Mi"

  # Local backend - process on non-standard host/port
  - id: "llama-custom-host"
    type: "local"
    command: "llama-server -m ./model.gguf --host 172.16.0.2 --port 8080"
    upstream_url: "http://172.16.0.2:8080"
    resources:
      memory: "200Mi"
    upstream_timeout: 90
    startup_timeout: 180  # Optional, wait up to 3 minutes for slow-starting backends
    # Chat completions will be: http://172.16.0.2:8080/v1/chat/completions
    # Health check will be: http://172.16.0.2:8080/health

  # Local embeddings backend - for text embeddings only (NOT chat completions)
  # This backend type can only serve /v1/embeddings requests
  - id: "nomic-embed-text"
    type: "local-embeddings"
    command: "llama-embedding -m ./nomic-embed-text-v1.5.Q8_0.gguf --port {port} --host 127.0.0.1"
    resources:
      memory: "500Mi"
    upstream_timeout: 30
    # Embeddings will be: http://127.0.0.1:{port}/v1/embeddings


resources:
  total_memory: "1024Mi"
